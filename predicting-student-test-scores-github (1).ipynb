{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T19:32:26.237736Z","iopub.execute_input":"2026-01-12T19:32:26.237942Z","iopub.status.idle":"2026-01-12T19:32:28.120646Z","shell.execute_reply.started":"2026-01-12T19:32:26.237922Z","shell.execute_reply":"2026-01-12T19:32:28.119521Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s6e1/sample_submission.csv\n/kaggle/input/playground-series-s6e1/train.csv\n/kaggle/input/playground-series-s6e1/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Accessing the training and and testing datasets\nX_data =pd.read_csv('/kaggle/input/playground-series-s6e1/train.csv')\nX_data_test = pd.read_csv('/kaggle/input/playground-series-s6e1/test.csv')\n\n#selecting the target for the predictions\ny = X_data['exam_score'].copy()\n\n#Selecting the prospective predictors for exam scores\nfeatures = ['age', 'gender', 'course','study_hours','class_attendance','internet_access','sleep_hours','sleep_quality','study_method','facility_rating']\nX = X_data[features].copy()\nX_test = X_data_test[features].copy()\n\n#Splitting the data into the their respective training and validation sets. \nX_train, X_valid, y_train, y_valid = train_test_split(X,y, train_size =0.8, test_size=0.2, random_state= 0)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T19:44:25.207524Z","iopub.execute_input":"2026-01-12T19:44:25.207936Z","iopub.status.idle":"2026-01-12T19:44:26.036910Z","shell.execute_reply.started":"2026-01-12T19:44:25.207900Z","shell.execute_reply":"2026-01-12T19:44:26.036253Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#Selecting desired numerical and categorical columns\ncategorical_cols2 = [colname for colname in X_train.columns if \n                   X_train[colname].nunique()<8 and\n                   X_train[colname].dtype=='object']\nnumerical_cols2 = [colname for colname in X_train.columns if \n                  X_train[colname].dtype in ['int64', 'float64']]\nfull_cols = categorical_cols2 + numerical_cols2\nX_train_full = X_train[full_cols].copy()\nX_valid_full= X_valid[full_cols].copy()\nX_test_full = X_test[full_cols].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T21:06:11.564589Z","iopub.execute_input":"2026-01-12T21:06:11.564883Z","iopub.status.idle":"2026-01-12T21:06:11.786393Z","shell.execute_reply.started":"2026-01-12T21:06:11.564864Z","shell.execute_reply":"2026-01-12T21:06:11.785504Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"#Importing the OneHotEncoder and the SimpleImputer for imputing and encoding\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T21:06:14.586187Z","iopub.execute_input":"2026-01-12T21:06:14.587229Z","iopub.status.idle":"2026-01-12T21:06:14.591582Z","shell.execute_reply.started":"2026-01-12T21:06:14.587194Z","shell.execute_reply":"2026-01-12T21:06:14.590286Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"#excluding outliers\ny_other = y_train  \n\nq1, q3 = np.percentile(y_other, [25, 75])\niqr = q3 - q1\n\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n\noutlier_mask = (y_other < lower) | (y_other> upper)\nx_train_clean =X_train_full.loc[~outlier_mask]\ny_train_clean =y_other.loc[~outlier_mask]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T21:06:18.656957Z","iopub.execute_input":"2026-01-12T21:06:18.657229Z","iopub.status.idle":"2026-01-12T21:06:18.719219Z","shell.execute_reply.started":"2026-01-12T21:06:18.657210Z","shell.execute_reply":"2026-01-12T21:06:18.718207Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"#OneHotEncoding\n#Encoder\n\n#selecting potential data leaks and removing the features that could cause data leakage\npotential_leaks= ['exam_difficulty', 'age', 'course', 'internet_activity','gender']\nX_train_drop = x_train_clean.drop(columns = [i for i in potential_leaks2 if i in x_train_clean.columns])\nX_valid_drop = X_valid_full.drop(columns = [i for i in potential_leaks2 if i in X_valid_full.columns])\nX_test_drop =X_test_full.drop(columns =[i for i in potential_leaks2 if i in X_test_full.columns])\ns = (X_train_drop.dtypes == 'object')\nobject_cols2 = list(s[s].index)\nt = (X_valid_drop.dtypes == 'object')\nobject_cols3 = list(t[t].index)\n\n#OneHotEcoder on the data without data leaks\nOH = OneHotEncoder(handle_unknown= \"ignore\", sparse_output = False)\nOH_X_train_cols = pd.DataFrame(OH.fit_transform(X_train_drop[object_cols2]))\nOH_X_valid_cols = pd.DataFrame(OH.transform(X_valid_drop[object_cols3]))\n\nOH_X_train_cols.index = X_train_drop.index\nOH_X_valid_cols.index = X_valid_drop.index\n\ndrop_Xt = X_train_drop.drop(object_cols2, axis=1)\ndrop_Xv = X_valid_drop.drop(object_cols3, axis=1)\n\nOH_X_train = pd.concat([drop_Xt, OH_X_train_cols], axis=1)\nOH_X_valid = pd.concat([drop_Xv, OH_X_valid_cols], axis =1) \n\nOH_X_train.columns = OH_X_train.columns.astype(str)\nOH_X_valid.columns =OH_X_valid.columns.astype(str)\n\nOH_X_test =pd.get_dummies(X_test_drop)\nOH_X_test = OH_X_test.reindex(columns =OH_X_train.columns, fill_value =0)\n\n\n#SimpleImputer on the data \ns_i = SimpleImputer(strategy ='most_frequent')\nimputed_X_train = pd.DataFrame(s_i.fit_transform(OH_X_train))\nimputed_X_valid = pd.DataFrame(s_i.transform(OH_X_valid))\nimputed_X_train.columns= OH_X_train.columns\nimputed_X_valid.columns= OH_X_valid.columns\nimputed_X_test =pd.DataFrame(s_i.transform(OH_X_test), columns =OH_X_train.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T21:28:04.832004Z","iopub.execute_input":"2026-01-12T21:28:04.832397Z","iopub.status.idle":"2026-01-12T21:28:05.562910Z","shell.execute_reply.started":"2026-01-12T21:28:04.832374Z","shell.execute_reply":"2026-01-12T21:28:05.561947Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"#Importing XGBoost and root_mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import root_mean_squared_error\n \n#Combining training and validation for full dataset\nX_final = np.vstack([imputed_X_train,imputed_X_valid])\ny_final = np.hstack([y_train_clean,y_valid])\n\n#function computed to print the rmse score\ndef score_data(X_train,y_train,X_valid , y_valid):\n    model = XGBRegressor(n_estimators=860, \n                          max_depth=8, \n                          learning_rate=0.032, \n                          booster ='gbtree', \n                          objective =\"reg:squarederror\", \n                          eval_metric ='rmse', \n                          subsample=0.86,\n                          colsample_bytree=0.8,\n                          reg_lambda=5,\n                          reg_alpha=0.4,\n                          random_state=41,\n)\n    model.fit(X_train, \n              y_train, \n              eval_set= [(X_valid, y_valid)], \n              verbose= False)\n    preds = model.predict(X_valid)\n    return root_mean_squared_error(y_valid, preds)    \n    \nprint(score_data(imputed_X_train,y_train_clean,imputed_X_valid , y_valid))\n\n#The final XGBoost model for the full dataset\nmodel_full = XGBRegressor(n_estimators=860, \n                          max_depth=8, \n                          learning_rate=0.032, \n                          booster ='gbtree', \n                          objective =\"reg:squarederror\", \n                          eval_metric ='rmse', \n                          subsample=0.86,\n                          colsample_bytree=0.8,\n                          reg_lambda=5,\n                          reg_alpha=0.4,\n                          random_state=41,\n)\nmodel_full.fit(X_final, y_final, verbose= False)\npreds2 = model_full.predict(imputed_X_test)\n#printing the first 5 results/prediction\nprint(preds2[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T22:56:12.041747Z","iopub.execute_input":"2026-01-12T22:56:12.043489Z","iopub.status.idle":"2026-01-12T22:57:06.350354Z","shell.execute_reply.started":"2026-01-12T22:56:12.043452Z","shell.execute_reply":"2026-01-12T22:57:06.348239Z"}},"outputs":[{"name":"stdout","text":"8.750436901450229\n[72.68659  70.907814 84.70799  53.743515 43.741096]\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"#Importing the libraries for pipelining and cross-validation\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.compose import ColumnTransformer, make_column_selector as selector\n\n#selecting potential leaks\npotential_leaks= [ 'exam_difficulty', 'age', 'course', 'internet_activity','gender']\n\n#dropping potential leaks\nX_train_drop = X_train_full.drop(columns = [i for i in potential_leaks if i in X_train_full.columns])\n\nnumeric_cols= X_train_drop.select_dtypes(include =[\"int64\", \"float64\"]).columns\ncategorical_cols=X_train_drop.select_dtypes(include =[\"object\", \"category\", \"bool\"]).columns\n\n#preprocessing of data\npreprocess = ColumnTransformer(\n    transformers =[\n        (\"num\", SimpleImputer(strategy =\"median\"), numeric_cols),\n        (\"cat\", Pipeline(steps=[(\"imputer\", SimpleImputer(strategy =\"most_frequent\")), (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), categorical_cols),\n    ]\n)\n#pipeline model \nmodel_pipe = XGBRegressor(n_estimators=860, \n                          max_depth=8, \n                          learning_rate=0.032, \n                          booster ='gbtree', \n                          objective =\"reg:squarederror\", \n                          eval_metric ='rmse', \n                          subsample=0.86,\n                          colsample_bytree=0.8,\n                          reg_lambda=5,\n                          reg_alpha=0.4,\n                          random_state=49)\npipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", model_pipe)])\n#computing cross-validation scores\ncv_scores = cross_val_score(pipe,\n                            X_train_drop, \n                            y_train_clean, \n                            cv=5, \n                            scoring ='neg_root_mean_squared_error', \n                            error_score ='raise')\nprint(\"Cross vals: \", -cv_scores.mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T22:53:17.763165Z","iopub.execute_input":"2026-01-12T22:53:17.764145Z","iopub.status.idle":"2026-01-12T22:54:41.322207Z","shell.execute_reply.started":"2026-01-12T22:53:17.764113Z","shell.execute_reply":"2026-01-12T22:54:41.321343Z"}},"outputs":[{"name":"stdout","text":"Cross vals:  8.759446092668204\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"#### function computed to ascertain optimal hyperparameters\ndef get_score_opt(n_estimators):\n    pipe_line_second = Pipeline(steps=[\n        ('preprocessor', SimpleImputer()),\n        ('model', XGBRegressor(\n                          random_state=41,\n                          learning_rate=0.032,\n                          n_estimators= n_estimators, \n                          max_depth= 8, \n                          booster ='gbtree', \n                          objective = 'reg:squarederror', \n                          eval_metric ='rmse', \n                          subsample= 0.86,\n                          colsample_bytree=0.8,\n                          reg_lambda=5,\n                          reg_alpha=0.4\n                          ))\n    ])\n    avg_mse = -1* cross_val_score(\n        pipe_line_second, \n        imputed_X_train, \n        y_train,\n        cv =5, \n        scoring= 'neg_root_mean_squared_error', \n        error_score= 'raise'\n    )\n    return avg_mse.mean()\n#iteration to display the behaviour of the model of the specified function parameter  \nrange_opt = {}\nfor i in range(1,10):\n    iterate = 10*i +800\n    range_opt[iterate] = get_score_opt(iterate)\n    print(iterate, range_opt[iterate])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T22:14:43.079626Z","iopub.execute_input":"2026-01-12T22:14:43.079885Z"}},"outputs":[{"name":"stdout","text":"810 8.758324409870072\n820 8.75830224441226\n830 8.758284716238776\n840 8.758309710412915\n850 8.75826253201598\n860 8.758254227070084\n870 8.758328347546989\n880 8.758286636476607\n890 8.758339392203704\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\n\n#creating a notebook output/dataset\nsample =pd.read_csv('/kaggle/input/playground-series-s6e1/sample_submission.csv')\n\n\noutput = sample.copy()\noutput['exam_score'] = preds2\noutput.to_csv('output5.csv', index = False)\n\nprint(output.head)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T22:57:55.124139Z","iopub.execute_input":"2026-01-12T22:57:55.125133Z","iopub.status.idle":"2026-01-12T22:57:55.392207Z","shell.execute_reply.started":"2026-01-12T22:57:55.125094Z","shell.execute_reply":"2026-01-12T22:57:55.390782Z"}},"outputs":[{"name":"stdout","text":"<bound method NDFrame.head of             id  exam_score\n0       630000   72.686592\n1       630001   70.907814\n2       630002   84.707993\n3       630003   53.743515\n4       630004   43.741096\n...        ...         ...\n269995  899995   58.845261\n269996  899996   37.589554\n269997  899997   75.613831\n269998  899998   58.394192\n269999  899999   70.382912\n\n[270000 rows x 2 columns]>\n","output_type":"stream"}],"execution_count":80}]}